{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQk2oFFYVfD5WAnem40dhP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masterjun12/Skill/blob/main/DNN_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SdZQcfLRHaXZ",
        "outputId": "68b464b1-b655-453a-cf75-7e6d0591bb16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1 \t batch: 1 \t loss: \t 3.8959405772009843\t정확도: 12.183333333333334\n",
            "epoch:1 \t batch: 2 \t loss: \t 3.438017324428408\t정확도: 14.716666666666667\n",
            "epoch:1 \t batch: 3 \t loss: \t 3.235658805048187\t정확도: 13.883333333333333\n",
            "epoch:1 \t batch: 4 \t loss: \t 3.169565733327532\t정확도: 15.483333333333333\n",
            "epoch:1 \t batch: 5 \t loss: \t 3.135273567322882\t정확도: 18.71666666666667\n",
            "epoch:1 \t batch: 6 \t loss: \t 3.090188263557399\t정확도: 23.333333333333332\n",
            "epoch:1 \t batch: 7 \t loss: \t 3.0370884521471946\t정확도: 26.8\n",
            "epoch:1 \t batch: 8 \t loss: \t 2.9906753231094196\t정확도: 30.516666666666666\n",
            "epoch:1 \t batch: 9 \t loss: \t 2.937120481608016\t정확도: 33.31666666666667\n",
            "epoch:1 \t batch: 10 \t loss: \t 2.8417386460064322\t정확도: 37.71666666666667\n",
            "epoch:2 \t batch: 1 \t loss: \t 2.759748214273094\t정확도: 39.61666666666667\n",
            "epoch:2 \t batch: 2 \t loss: \t 2.682421406582395\t정확도: 40.61666666666667\n",
            "epoch:2 \t batch: 3 \t loss: \t 2.6227671174513256\t정확도: 40.31666666666667\n",
            "epoch:2 \t batch: 4 \t loss: \t 2.528125308909367\t정확도: 42.88333333333333\n",
            "epoch:2 \t batch: 5 \t loss: \t 2.504477827519902\t정확도: 41.766666666666666\n",
            "epoch:2 \t batch: 6 \t loss: \t 2.796952902934953\t정확도: 35.18333333333333\n",
            "epoch:2 \t batch: 7 \t loss: \t 3.1161130333318154\t정확도: 34.21666666666667\n",
            "epoch:2 \t batch: 8 \t loss: \t 2.6285158598675897\t정확도: 38.666666666666664\n",
            "epoch:2 \t batch: 9 \t loss: \t 2.3654445502696864\t정확도: 46.800000000000004\n",
            "epoch:2 \t batch: 10 \t loss: \t 2.1622433574787627\t정확도: 52.666666666666664\n",
            "epoch:3 \t batch: 1 \t loss: \t 2.0936856164212485\t정확도: 54.949999999999996\n",
            "epoch:3 \t batch: 2 \t loss: \t 2.0919404358406926\t정확도: 52.5\n",
            "epoch:3 \t batch: 3 \t loss: \t 2.5232455543706287\t정확도: 42.733333333333334\n",
            "epoch:3 \t batch: 4 \t loss: \t 3.5123711728834097\t정확도: 33.86666666666667\n",
            "epoch:3 \t batch: 5 \t loss: \t 3.079365600678638\t정확도: 28.299999999999997\n",
            "epoch:3 \t batch: 6 \t loss: \t 2.755588131064503\t정확도: 35.66666666666667\n",
            "epoch:3 \t batch: 7 \t loss: \t 2.542165383309151\t정확도: 42.15\n",
            "epoch:3 \t batch: 8 \t loss: \t 2.3702210806855772\t정확도: 50.63333333333333\n",
            "epoch:3 \t batch: 9 \t loss: \t 2.20154409845292\t정확도: 54.78333333333333\n",
            "epoch:3 \t batch: 10 \t loss: \t 2.026605868867329\t정확도: 58.35\n",
            "epoch:4 \t batch: 1 \t loss: \t 1.9479500265513177\t정확도: 58.96666666666667\n",
            "epoch:4 \t batch: 2 \t loss: \t 1.899568239374673\t정확도: 60.483333333333334\n",
            "epoch:4 \t batch: 3 \t loss: \t 2.2171274271986188\t정확도: 50.21666666666667\n",
            "epoch:4 \t batch: 4 \t loss: \t 2.9955667559720625\t정확도: 34.983333333333334\n",
            "epoch:4 \t batch: 5 \t loss: \t 2.598894419973591\t정확도: 44.166666666666664\n",
            "epoch:4 \t batch: 6 \t loss: \t 2.2668437446810157\t정확도: 45.766666666666666\n",
            "epoch:4 \t batch: 7 \t loss: \t 2.1226637691099612\t정확도: 48.88333333333333\n",
            "epoch:4 \t batch: 8 \t loss: \t 2.0284648770490095\t정확도: 51.38333333333334\n",
            "epoch:4 \t batch: 9 \t loss: \t 1.907588991246886\t정확도: 57.36666666666667\n",
            "epoch:4 \t batch: 10 \t loss: \t 1.7009942490978531\t정확도: 63.733333333333334\n",
            "epoch:5 \t batch: 1 \t loss: \t 1.660959410367435\t정확도: 64.56666666666668\n",
            "epoch:5 \t batch: 2 \t loss: \t 1.733914674592064\t정확도: 61.36666666666667\n",
            "epoch:5 \t batch: 3 \t loss: \t 2.518884907036152\t정확도: 46.88333333333333\n",
            "epoch:5 \t batch: 4 \t loss: \t 2.664657902219491\t정확도: 46.666666666666664\n",
            "epoch:5 \t batch: 5 \t loss: \t 2.246078496586368\t정확도: 46.916666666666664\n",
            "epoch:5 \t batch: 6 \t loss: \t 2.0605323666046345\t정확도: 56.08333333333333\n",
            "epoch:5 \t batch: 7 \t loss: \t 1.7784364954142402\t정확도: 61.96666666666667\n",
            "epoch:5 \t batch: 8 \t loss: \t 1.6646037676254382\t정확도: 63.849999999999994\n",
            "epoch:5 \t batch: 9 \t loss: \t 1.5628201423243224\t정확도: 68.73333333333333\n",
            "epoch:5 \t batch: 10 \t loss: \t 1.406285013885416\t정확도: 71.15\n",
            "epoch:6 \t batch: 1 \t loss: \t 1.4395964923642672\t정확도: 70.33333333333334\n",
            "epoch:6 \t batch: 2 \t loss: \t 1.4547090635716586\t정확도: 69.26666666666667\n",
            "epoch:6 \t batch: 3 \t loss: \t 1.7846338668146937\t정확도: 60.46666666666667\n",
            "epoch:6 \t batch: 4 \t loss: \t 2.149930919347398\t정확도: 54.85\n",
            "epoch:6 \t batch: 5 \t loss: \t 2.6159263910854325\t정확도: 47.16666666666667\n",
            "epoch:6 \t batch: 6 \t loss: \t 2.7250627292768153\t정확도: 38.550000000000004\n",
            "epoch:6 \t batch: 7 \t loss: \t 2.0037420782251623\t정확도: 53.449999999999996\n",
            "epoch:6 \t batch: 8 \t loss: \t 1.7218196680432072\t정확도: 62.26666666666667\n",
            "epoch:6 \t batch: 9 \t loss: \t 1.5202162105544215\t정확도: 69.1\n",
            "epoch:6 \t batch: 10 \t loss: \t 1.3180888670420403\t정확도: 74.61666666666666\n",
            "epoch:7 \t batch: 1 \t loss: \t 1.3344750694836525\t정확도: 73.53333333333333\n",
            "epoch:7 \t batch: 2 \t loss: \t 1.2830976973330972\t정확도: 74.58333333333333\n",
            "epoch:7 \t batch: 3 \t loss: \t 1.4350846025305815\t정확도: 69.93333333333334\n",
            "epoch:7 \t batch: 4 \t loss: \t 1.470126925038407\t정확도: 68.7\n",
            "epoch:7 \t batch: 5 \t loss: \t 1.7687361090805176\t정확도: 62.74999999999999\n",
            "epoch:7 \t batch: 6 \t loss: \t 1.8943859621884385\t정확도: 60.86666666666667\n",
            "epoch:7 \t batch: 7 \t loss: \t 1.5596595650687997\t정확도: 66.56666666666666\n",
            "epoch:7 \t batch: 8 \t loss: \t 1.39495176648578\t정확도: 72.16666666666667\n",
            "epoch:7 \t batch: 9 \t loss: \t 1.2822136731242681\t정확도: 74.7\n",
            "epoch:7 \t batch: 10 \t loss: \t 1.1618732948457653\t정확도: 76.88333333333334\n",
            "epoch:8 \t batch: 1 \t loss: \t 1.2519298058661577\t정확도: 73.58333333333333\n",
            "epoch:8 \t batch: 2 \t loss: \t 1.340073724143948\t정확도: 71.53333333333333\n",
            "epoch:8 \t batch: 3 \t loss: \t 1.618999115927559\t정확도: 66.08333333333334\n",
            "epoch:8 \t batch: 4 \t loss: \t 1.6329804630435076\t정확도: 64.46666666666667\n",
            "epoch:8 \t batch: 5 \t loss: \t 1.480098731212215\t정확도: 66.91666666666667\n",
            "epoch:8 \t batch: 6 \t loss: \t 1.4927343766262984\t정확도: 68.16666666666666\n",
            "epoch:8 \t batch: 7 \t loss: \t 1.385357230401501\t정확도: 69.81666666666668\n",
            "epoch:8 \t batch: 8 \t loss: \t 1.3768250781739315\t정확도: 70.83333333333334\n",
            "epoch:8 \t batch: 9 \t loss: \t 1.4536155583214483\t정확도: 68.43333333333334\n",
            "epoch:8 \t batch: 10 \t loss: \t 1.3034775000134735\t정확도: 72.91666666666666\n",
            "epoch:9 \t batch: 1 \t loss: \t 1.276033421742795\t정확도: 73.15\n",
            "epoch:9 \t batch: 2 \t loss: \t 1.2377254584251263\t정확도: 75.06666666666668\n",
            "epoch:9 \t batch: 3 \t loss: \t 1.363773874904161\t정확도: 71.39999999999999\n",
            "epoch:9 \t batch: 4 \t loss: \t 1.2187104942609368\t정확도: 74.65\n",
            "epoch:9 \t batch: 5 \t loss: \t 1.259517612441137\t정확도: 73.23333333333333\n",
            "epoch:9 \t batch: 6 \t loss: \t 1.434798889048454\t정확도: 69.13333333333334\n",
            "epoch:9 \t batch: 7 \t loss: \t 1.5365120666071646\t정확도: 66.4\n",
            "epoch:9 \t batch: 8 \t loss: \t 1.5138693022509417\t정확도: 68.25\n",
            "epoch:9 \t batch: 9 \t loss: \t 1.3110034908382224\t정확도: 72.76666666666667\n",
            "epoch:9 \t batch: 10 \t loss: \t 1.0566543008651808\t정확도: 79.80000000000001\n",
            "epoch:10 \t batch: 1 \t loss: \t 1.0834153817837893\t정확도: 77.93333333333334\n",
            "epoch:10 \t batch: 2 \t loss: \t 1.0896388624310154\t정확도: 79.06666666666666\n",
            "epoch:10 \t batch: 3 \t loss: \t 1.262303054822439\t정확도: 74.11666666666666\n",
            "epoch:10 \t batch: 4 \t loss: \t 1.2634054240369141\t정확도: 74.21666666666667\n",
            "epoch:10 \t batch: 5 \t loss: \t 1.3754713274279202\t정확도: 69.78333333333333\n",
            "epoch:10 \t batch: 6 \t loss: \t 1.592104947153241\t정확도: 66.66666666666666\n",
            "epoch:10 \t batch: 7 \t loss: \t 1.438796681461857\t정확도: 69.28333333333333\n",
            "epoch:10 \t batch: 8 \t loss: \t 1.2845662166309577\t정확도: 74.03333333333333\n",
            "epoch:10 \t batch: 9 \t loss: \t 1.1003954683166297\t정확도: 79.14999999999999\n",
            "epoch:10 \t batch: 10 \t loss: \t 0.9151590376551096\t정확도: 82.8\n",
            "epoch:11 \t batch: 1 \t loss: \t 0.9882967827762446\t정확도: 80.53333333333333\n",
            "epoch:11 \t batch: 2 \t loss: \t 1.0091498905577894\t정확도: 80.11666666666667\n",
            "epoch:11 \t batch: 3 \t loss: \t 1.2467771181053837\t정확도: 73.66666666666667\n",
            "epoch:11 \t batch: 4 \t loss: \t 1.2062260807532597\t정확도: 75.16666666666667\n",
            "epoch:11 \t batch: 5 \t loss: \t 1.3571081296474246\t정확도: 71.89999999999999\n",
            "epoch:11 \t batch: 6 \t loss: \t 1.2195332268983756\t정확도: 75.11666666666666\n",
            "epoch:11 \t batch: 7 \t loss: \t 1.035734702620802\t정확도: 79.86666666666666\n",
            "epoch:11 \t batch: 8 \t loss: \t 1.0433552438365716\t정확도: 79.5\n",
            "epoch:11 \t batch: 9 \t loss: \t 1.0681265229779096\t정확도: 78.73333333333333\n",
            "epoch:11 \t batch: 10 \t loss: \t 1.0348366728617813\t정확도: 78.93333333333334\n",
            "epoch:12 \t batch: 1 \t loss: \t 1.2208592759520671\t정확도: 74.4\n",
            "epoch:12 \t batch: 2 \t loss: \t 1.3532453352308838\t정확도: 72.39999999999999\n",
            "epoch:12 \t batch: 3 \t loss: \t 1.255628084508411\t정확도: 72.86666666666667\n",
            "epoch:12 \t batch: 4 \t loss: \t 1.0065828118495097\t정확도: 80.9\n",
            "epoch:12 \t batch: 5 \t loss: \t 0.9547850963274895\t정확도: 81.21666666666667\n",
            "epoch:12 \t batch: 6 \t loss: \t 1.018774477929716\t정확도: 80.01666666666667\n",
            "epoch:12 \t batch: 7 \t loss: \t 0.9500611001124352\t정확도: 81.71666666666667\n",
            "epoch:12 \t batch: 8 \t loss: \t 1.0156866607787307\t정확도: 80.05\n",
            "epoch:12 \t batch: 9 \t loss: \t 1.0917660476340092\t정확도: 78.08333333333334\n",
            "epoch:12 \t batch: 10 \t loss: \t 1.2028535200402175\t정확도: 76.25\n",
            "epoch:13 \t batch: 1 \t loss: \t 1.3909044982071155\t정확도: 71.61666666666666\n",
            "epoch:13 \t batch: 2 \t loss: \t 1.5106949853954648\t정확도: 68.96666666666667\n",
            "epoch:13 \t batch: 3 \t loss: \t 1.2765922550431288\t정확도: 73.58333333333333\n",
            "epoch:13 \t batch: 4 \t loss: \t 1.0301585053855924\t정확도: 80.45\n",
            "epoch:13 \t batch: 5 \t loss: \t 0.9668894975362267\t정확도: 81.71666666666667\n",
            "epoch:13 \t batch: 6 \t loss: \t 1.034241076170629\t정확도: 80.66666666666666\n",
            "epoch:13 \t batch: 7 \t loss: \t 1.008820220271633\t정확도: 79.93333333333334\n",
            "epoch:13 \t batch: 8 \t loss: \t 1.0620315380588523\t정확도: 78.88333333333334\n",
            "epoch:13 \t batch: 9 \t loss: \t 1.1079582624073696\t정확도: 77.61666666666667\n",
            "epoch:13 \t batch: 10 \t loss: \t 1.0279964140725006\t정확도: 78.8\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1a2c52c13d6f>\u001b[0m in \u001b[0;36m<cell line: 369>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# 역전파 및 확률적 경사 하강법 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mcomplete_NN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_testing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mcomplete_NN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplying_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mii\u001b[0m  \u001b[0;31m# 다음 배치의 시작 인덱스로 이동\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1a2c52c13d6f>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, Y)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 각 레이어를 역순으로 통해 역전파 진행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# 확률적 경사 하강법(SGD)을 적용하는 메서드 정의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1a2c52c13d6f>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, pooled)\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0mcheated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooled_transpose_re\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# 역전파 결과 갱신\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcheated\u001b[0m  \u001b[0;31m# 역전파 결과 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "fac = 5\n",
        "Mnist = tf.keras.datasets.mnist\n",
        "\n",
        "class Linear_Layer:\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, alpha=0.01, Theta=None, bias=None):\n",
        "        self.alpha = alpha  # 학습률(alpha) 초기화\n",
        "\n",
        "        # Theta(가중치 행렬)가 주어지지 않으면 무작위로 생성\n",
        "        if Theta is None:\n",
        "            self.Theta = np.random.randn(in_dim, out_dim) / fac  # 무작위 가중치 행렬 생성\n",
        "        else:\n",
        "            self.Theta = Theta  # 주어진 Theta 사용\n",
        "\n",
        "        # bias(편향 벡터)가 주어지지 않으면 무작위로 생성\n",
        "        if bias is None:\n",
        "            self.bias = np.random.randn(out_dim) / fac  # 무작위 편향 벡터 생성\n",
        "        else:\n",
        "            self.bias = bias  # 주어진 bias 사용\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, X):\n",
        "        self.X = X\n",
        "        self.z = np.matmul(X, self.Theta) + self.bias  # 선형 변환 계산\n",
        "        return self.z\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, grad_previous):\n",
        "        t = self.X.shape[0]  # 입력 데이터의 행 수\n",
        "        self.grad = np.matmul(self.X.transpose(), grad_previous) / t  # Theta에 대한 그래디언트 계산\n",
        "        self.grad_bias = grad_previous.sum(axis=0) / t  # 편향에 대한 그래디언트 계산\n",
        "        self.grad_a = np.matmul(grad_previous, self.Theta.transpose())  # 입력에 대한 그래디언트 계산\n",
        "        return self.grad_a\n",
        "\n",
        "    # 확률적 경사 하강법(SGD)을 적용하는 메서드 정의\n",
        "    def applying_sgd(self):\n",
        "        self.Theta = self.Theta - (self.alpha * self.grad)  # Theta 업데이트\n",
        "        self.bias = self.bias - (self.alpha * self.grad_bias)  # 편향 업데이트\n",
        "\n",
        "class softmax:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # 원-핫 인코딩을 수행하는 메서드\n",
        "    def expansion(self, t):\n",
        "        (a,) = t.shape  # 입력 벡터의 길이(a)를 가져옴\n",
        "        Y = np.zeros((a, 10))  # 10개의 클래스에 대한 원-핫 인코딩을 위한 빈 행렬 생성\n",
        "        for i in range(0, a):\n",
        "            Y[i, t[i]] = 1  # 입력 벡터의 각 원소에 대해 해당하는 클래스 인덱스를 1로 설정\n",
        "        return Y\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, z):\n",
        "        self.z = z  # 입력값 저장\n",
        "        (p, t) = self.z.shape  # p: 입력 데이터의 개수, t: 클래스 수\n",
        "        self.a = np.zeros((p, t))  # 출력값을 저장할 배열 초기화\n",
        "        for i in range(0, p):\n",
        "            for ii in range(0, t):\n",
        "                # 소프트맥스 함수를 이용하여 클래스 확률 계산\n",
        "                self.a[i, ii] = (np.exp(self.z[i, ii])) / (np.sum(np.exp(self.z[i, :])))\n",
        "        return self.a  # 계산된 클래스 확률 반환\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, Y):\n",
        "        y = self.expansion(Y)  # 실제 클래스를 원-핫 인코딩으로 변환\n",
        "        self.grad = (self.a - y)  # 그래디언트 계산\n",
        "        return self.grad  # 그래디언트 반환\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass  # 확률적 경사 하강법(SGD)\n",
        "\n",
        "class relu:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, z):\n",
        "        if len(z.shape) == 3:  # 입력이 3차원인 경우\n",
        "            z_temp = z.reshape((z.shape[0], z.shape[1] * z.shape[2]))  # 3차원 배열을 2차원으로 변환\n",
        "            z_temp_1 = self.forward_pass(z_temp)  # 2차원 배열에 대해 순전파를 재귀적으로 호출\n",
        "            self.a_1 = z_temp_1.reshape((z.shape[0], z.shape[1], z.shape[2]))  # 결과를 다시 3차원으로 변환\n",
        "            return self.a_1\n",
        "\n",
        "        else:  # 입력이 2차원인 경우\n",
        "            (p, t) = z.shape  # p: 데이터 개수, t: 특성 수\n",
        "            self.a = np.zeros((p, t))  # 출력값을 저장할 배열 초기화\n",
        "            for i in range(0, p):\n",
        "                for ii in range(0, t):\n",
        "                    self.a[i, ii] = max([0, z[i, ii]])  # ReLU 활성화 함수 적용\n",
        "            return self.a\n",
        "\n",
        "    # ReLU 함수의 도함수를 계산하는 메서드 정의\n",
        "    def derivative(self, a):\n",
        "        if a > 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, grad_previous):\n",
        "        if len(grad_previous.shape) == 3:  # 입력이 3차원인 경우\n",
        "            (d, p, t) = grad_previous.shape\n",
        "            self.grad = np.zeros((d, p, t))  # 출력값에 대한 그래디언트를 저장할 배열 초기화\n",
        "\n",
        "            for i in range(d):\n",
        "                for ii in range(p):\n",
        "                    for iii in range(t):\n",
        "                        # ReLU 함수의 도함수를 적용한 그래디언트 계산\n",
        "                        self.grad[i, ii, iii] = (grad_previous[i, ii, iii] * self.derivative(self.a_1[i, ii, iii]))\n",
        "\n",
        "            return self.grad\n",
        "\n",
        "        else:  # 입력이 2차원인 경우\n",
        "            (p, t) = grad_previous.shape\n",
        "            self.grad = np.zeros((p, t))  # 출력값에 대한 그래디언트를 저장할 배열 초기화\n",
        "            for i in range(p):\n",
        "                for ii in range(t):\n",
        "                    # ReLU 함수의 도함수를 적용한 그래디언트 계산\n",
        "                    self.grad[i, ii] = grad_previous[i, ii] * self.derivative(self.a[i, ii])\n",
        "            return self.grad\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass  # 확률적 경사 하강법(SGD)\n",
        "\n",
        "\n",
        "class padding():\n",
        "\n",
        "    def __init__(self, pad=1):\n",
        "        self.pad = pad\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, data):\n",
        "        # 입력 데이터 주위에 패딩을 추가하여 출력 데이터 생성\n",
        "        X = np.pad(data, ((0, 0), (self.pad, self.pad), (self.pad, self.pad)), 'constant', constant_values=0)\n",
        "        return X\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, y):\n",
        "        # 출력 데이터의 패딩을 제거하고 반환\n",
        "        return y[:, 1:(y.shape[1] - 1), 1:(y.shape[2] - 1)]\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass  # 확률적 경사 하강법(SGD)\n",
        "\n",
        "class Convolutional_Layer:\n",
        "    def __init__(self, filter_dim=3, stride=1, pad=1, alpha=0.01):\n",
        "        self.filter_dim = filter_dim\n",
        "        self.stride = stride\n",
        "        self.filter = np.random.randn(self.filter_dim, self.filter_dim)\n",
        "        self.filter = self.filter / self.filter.sum()\n",
        "        self.bias = np.random.rand() / 10\n",
        "        self.pad = pad\n",
        "        self.alpha = alpha\n",
        "\n",
        "    # 컨볼루션 연산을 수행하는 메서드 정의\n",
        "    def convolving(self, X, fil, dimen_x, dimen_y):\n",
        "        z = np.zeros((dimen_x, dimen_y))\n",
        "        for i in range(dimen_x):\n",
        "            for ii in range(dimen_y):\n",
        "                temp = np.multiply(X[i: i + fil.shape[0], ii: ii + fil.shape[1]], fil)\n",
        "                z[i, ii] = temp.sum()\n",
        "        return z\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, X):\n",
        "        self.X = X\n",
        "        (d, p, t) = self.X.shape\n",
        "        dimen_x = int(((p - self.filter_dim) / self.stride) + 1)\n",
        "        dimen_y = int(((t - self.filter_dim) / self.stride) + 1)\n",
        "        self.z = np.zeros((d, dimen_x, dimen_y))\n",
        "        for i in range(d):\n",
        "            self.z[i] = (self.convolving(self.X[i], self.filter, dimen_x, dimen_y) + self.bias)\n",
        "\n",
        "        return self.z\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, grad_z):\n",
        "        (d, p, t) = grad_z.shape\n",
        "        filter_1 = np.flip((np.flip(self.filter, axis=0)), axis=1)\n",
        "        self.grads = np.zeros((d, p, t))\n",
        "        for i in range(d):\n",
        "            self.grads[i] = self.convolving(np.pad(grad_z[i], ((1, 1), (1, 1)), 'constant', constant_values=0),\n",
        "                                            filter_1, p, t)\n",
        "\n",
        "        self.grads = np.pad(self.grads, ((0, 0), (1, 1), (1, 1)), 'constant', constant_values=0)\n",
        "\n",
        "        self.grad_filter = np.zeros((self.filter_dim, self.filter_dim))\n",
        "\n",
        "        for i in range(self.filter_dim):\n",
        "            for ii in range(self.filter_dim):\n",
        "                self.grad_filter[i, ii] = (np.multiply(grad_z, self.X[:, i:p + i, ii:t + ii])).sum()\n",
        "        self.grad_filter = self.grad_filter / (d)\n",
        "\n",
        "        self.grad_bias = (grad_z.sum()) / (d)\n",
        "        return self.grads\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        # 확률적 경사 하강법(SGD)을 사용하여 가중치와 편향 업데이트\n",
        "        self.filter = self.filter - (self.alpha * self.grad_filter)\n",
        "        self.bias = self.bias - (self.alpha * self.grad_bias)\n",
        "\n",
        "class pooling:\n",
        "\n",
        "    def __init__(self, pool_dim=2, stride=2):\n",
        "        self.pool_dim = pool_dim  # 풀링 윈도우의 크기\n",
        "        self.stride = stride  # 스트라이드 값\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, data):\n",
        "        (q, p, t) = data.shape  # 입력 데이터의 형태\n",
        "        z_x = int((p - self.pool_dim) / self.stride) + 1  # x 축에 대한 풀링 결과 크기\n",
        "        z_y = int((t - self.pool_dim) / self.stride) + 1  # y 축에 대한 풀링 결과 크기\n",
        "        after_pool = np.zeros((q, z_x, z_y))  # 풀링 결과를 저장할 배열 초기화\n",
        "\n",
        "        for ii in range(0, q):  # 배치 개수만큼 반복\n",
        "            liss = []  # 풀링 결과를 저장할 리스트 초기화\n",
        "            for i in range(0, p, self.stride):\n",
        "                for j in range(0, t, self.stride):\n",
        "                    if (i + self.pool_dim <= p) and (j + self.pool_dim <= t):\n",
        "                        temp = data[ii, i:(i + self.pool_dim), j:(j + self.pool_dim)]\n",
        "                        temp_1 = np.max(temp)  # 최대값 풀링 연산 수행\n",
        "                        liss.append(temp_1)  # 결과를 리스트에 추가\n",
        "            liss = np.asarray(liss)  # 리스트를 NumPy 배열로 변환\n",
        "            liss = liss.reshape((z_x, z_y))  # 결과를 풀링 크기에 맞게 재구성\n",
        "            after_pool[ii] = liss  # 풀링 결과를 저장\n",
        "\n",
        "        return after_pool  # 풀링 결과 반환\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, pooled):\n",
        "        (a, b, c) = pooled.shape  # 풀링 결과의 형태\n",
        "        cheated = np.zeros((a, 2 * b, 2 * c))  # 역전파 결과를 저장할 배열 초기화\n",
        "\n",
        "        for k in range(0, a):  # 배치 개수만큼 반복\n",
        "            pooled_transpose_re = pooled[k].reshape((b * c))  # 풀링 결과를 벡터 형태로 재구성\n",
        "            count = 0  # 역전파 결과를 저장할 인덱스 초기화\n",
        "            for i in range(0, 2 * b, self.stride):\n",
        "                for j in range(0, 2 * c, self.stride):\n",
        "                    cheated[k, i:(i + self.stride), j:(j + self.stride)] = pooled_transpose_re[count]\n",
        "                    count = count + 1  # 역전파 결과 갱신\n",
        "\n",
        "        return cheated  # 역전파 결과 반환\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass  # 확률적 경사 하강법(SGD)\n",
        "\n",
        "class Neural_Network:\n",
        "\n",
        "    def __init__(self, Network):\n",
        "        self.Network = Network  # 네트워크 레이어들을 리스트로 받아 초기화\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, X):\n",
        "        n = X\n",
        "        for i in self.Network:\n",
        "            n = i.forward_pass(n)  # 각 레이어를 통해 순전파 진행\n",
        "\n",
        "        return n  # 최종 결과 반환\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, Y):\n",
        "        m = Y\n",
        "        for i in reversed(self.Network):\n",
        "            m = i.backprop(m)  # 각 레이어를 역순으로 통해 역전파 진행\n",
        "\n",
        "    # 확률적 경사 하강법(SGD)을 적용하는 메서드 정의\n",
        "    def applying_sgd(self):\n",
        "        for i in self.Network:\n",
        "            i.applying_sgd()  # 각 레이어에 대해 SGD 적용\n",
        "\n",
        "class reshaping:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, a):\n",
        "        self.shape_a = a.shape  # 입력 데이터의 형태(shape) 저장\n",
        "\n",
        "        # 입력 데이터를 2차원 형태로 재구성하여 반환\n",
        "        self.final_a = a.reshape(self.shape_a[0], self.shape_a[1] * self.shape_a[2])\n",
        "        return self.final_a\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, q):\n",
        "        # 역전파 시에는 입력 데이터의 형태를 원래 형태로 재구성하여 반환\n",
        "        return q.reshape(self.shape_a[0], self.shape_a[1], self.shape_a[2])\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass  # 확률적 경사 하강법(SGD)\n",
        "\n",
        "class cross_entropy:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # 원-핫 인코딩을 수행하는 메서드\n",
        "    def expansion(self, t):\n",
        "        (a,) = t.shape\n",
        "        Y = np.zeros((a, 10))  # 10개의 클래스에 대한 원-핫 인코딩을 위한 빈 행렬 생성\n",
        "        for i in range(0, a):\n",
        "            Y[i, t[i]] = 1  # 입력 벡터의 각 원소에 대해 해당하는 클래스 인덱스를 1로 설정\n",
        "        return Y\n",
        "\n",
        "    # 크로스 엔트로피 손실을 계산하는 메서드\n",
        "    def loss(self, A, Y):\n",
        "        exp_Y = self.expansion(Y)  # 실제 클래스를 원-핫 인코딩으로 변환\n",
        "        (u, i) = A.shape  # u: 데이터 개수, i: 클래스 수\n",
        "        loss_matrix = np.zeros((u, i))  # 손실 값을 저장할 배열 초기화\n",
        "        for j in range(u):\n",
        "            for jj in range(i):\n",
        "                if exp_Y[j, jj] == 0:\n",
        "                    loss_matrix[j, jj] = np.log(1 - A[j, jj])\n",
        "                else:\n",
        "                    loss_matrix[j, jj] = np.log(A[j, jj])\n",
        "\n",
        "        # 전체 손실의 평균을 계산하여 반환\n",
        "        return ((-(loss_matrix.sum())) / u)\n",
        "\n",
        "class accuracy:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # 정확도를 계산하는 메서드\n",
        "    def value(self, out, Y):\n",
        "        self.out = np.argmax(out, axis=1)  # 출력값(out)에서 가장 큰 값의 인덱스를 가져옴\n",
        "        p = self.out.shape[0]  # 데이터 개수\n",
        "        total = 0  # 정확하게 예측된 데이터 수를 저장하는 변수\n",
        "        for i in range(p):\n",
        "            if Y[i] == self.out[i]:  # 실제 클래스(Y)와 예측 클래스(self.out)가 일치하면\n",
        "                total += 1  # 정확하게 예측된 데이터 수를 증가\n",
        "        return total / p  # 정확도를 전체 데이터 개수로 나누어 반환\n",
        "\n",
        "(Xtr, Ytr), (Xte, Yte) = Mnist.load_data()\n",
        "X_testing = Xtr[:, :, :]\n",
        "Y_testing = Ytr[:]\n",
        "X_testing = X_testing/255  # 입력 데이터를 0과 1 사이로 스케일링\n",
        "al = 0.3  # 학습률(learning rate)\n",
        "stopper = 85.0  # 정확도가 이 값 이상이면 훈련 중단\n",
        "\n",
        "complete_NN = Neural_Network([\n",
        "\n",
        "                                padding(),\n",
        "                                Convolutional_Layer(),\n",
        "                                pooling(),\n",
        "                                relu(),\n",
        "                                padding(),\n",
        "                                Convolutional_Layer(),\n",
        "                                pooling(),\n",
        "                                relu(),\n",
        "                                reshaping(),\n",
        "                                Linear_Layer(7*7, 24, alpha = al), #선형 레이어 - 7x7 크기의 입력을 24개의 출력으로 변환합니다.\n",
        "                                relu(),\n",
        "                                Linear_Layer(24, 10, alpha = al), #또 다른 선형 레이어 - 24개의 입력을 10개의 출력으로 변환합니다.\n",
        "                                softmax()\n",
        "\n",
        "                                ])\n",
        "CE = cross_entropy() #크로스 엔트로피 손실 함수\n",
        "\n",
        "acc = accuracy()  # 정확도 계산 객체 생성\n",
        "epochs = 100  # 총 에포크 수\n",
        "broke = 0  # 정확도가 stopper 이상인 경우 훈련 중단 플래그\n",
        "batches = 6000  # 각 배치의 크기\n",
        "\n",
        "for i in range(epochs):  # 에포크 반복\n",
        "    k = 0  # 데이터 인덱스 초기화\n",
        "    for ii in range(batches, 60001, batches):  # 배치 크기로 데이터를 나누어 처리\n",
        "\n",
        "        # 순전파 수행하여 손실 및 정확도 계산\n",
        "        out = complete_NN.forward_pass(X_testing[k:ii])\n",
        "        print(\"epoch:{} \\t batch: {} \\t loss: \\t {}\".format(i+1, int(ii/batches), CE.loss(out, Y_testing[k:ii])), end=\"\\t\")\n",
        "        accur = acc.value(out, Y_testing[k:ii])*100\n",
        "        print(\"정확도: {}\".format(accur))\n",
        "\n",
        "        if accur >= stopper:  # 정확도가 stopper 이상이면 훈련 중단\n",
        "            broke = 1\n",
        "            break\n",
        "\n",
        "        # 역전파 및 확률적 경사 하강법 적용\n",
        "        complete_NN.backprop(Y_testing[k:ii])\n",
        "        complete_NN.applying_sgd()\n",
        "        k = ii  # 다음 배치의 시작 인덱스로 이동\n",
        "\n",
        "    if broke == 1:  # 정확도가 stopper 이상인 경우 훈련 중단\n",
        "        break\n",
        "\n",
        "# 훈련된 신경망 모델을 사용하여 훈련 데이터에 대한 결과 계산\n",
        "out = complete_NN.forward_pass(X_testing)\n",
        "\n",
        "# 훈련 데이터에 대한 최종 손실을 계산하고 출력\n",
        "print(\"트레인 로스 {}\".format(CE.loss(out, Y_testing)))\n",
        "\n",
        "# 훈련 데이터에 대한 정확도를 계산하고 출력\n",
        "print(\"트레인 정확도 {}\".format(acc.value(out, Y_testing)*100))\n",
        "\n",
        "# 테스트 데이터를 전처리 (스케일링)\n",
        "Xtest = Xte/255\n",
        "\n",
        "# 테스트 데이터에 대한 결과 계산\n",
        "out_1 = complete_NN.forward_pass(Xtest)\n",
        "\n",
        "# 테스트 데이터에 대한 정확도를 계산하고 출력\n",
        "print(\"테스트 정확도 {}\".format(acc.value(out_1, Yte)*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# 랜덤한 인덱스 선택\n",
        "random_index = np.random.randint(0, len(x_test))\n",
        "random_image = x_test[random_index]\n",
        "random_label = y_test[random_index]\n",
        "\n",
        "# 선택한 이미지를 모델에 입력하기 위해 전처리\n",
        "input_image = random_image.reshape(1, 28, 28) / 255.0  # 이미지 스케일링 및 형태 조정\n",
        "\n",
        "# 모델 예측\n",
        "predictions = complete_NN.forward_pass(input_image)\n",
        "\n",
        "# 예측 결과에서 가장 높은 확률을 갖는 클래스 선택\n",
        "predicted_label = np.argmax(predictions)\n",
        "\n",
        "# 이미지 출력\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(random_image, cmap='gray')\n",
        "plt.title(f\"True Label: {random_label}, Predicted Label: {predicted_label}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "1TZBXCDMTFnM",
        "outputId": "5302655b-820b-443b-80fe-98d0ef04b7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f99e8e42ed7e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# MNIST 데이터셋 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 랜덤한 인덱스 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    }
  ]
}